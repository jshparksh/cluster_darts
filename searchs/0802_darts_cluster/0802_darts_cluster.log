08/02 06:36:56 AM | 
08/02 06:36:56 AM | Parameters:
08/02 06:36:56 AM | ALPHA_LR=0.0003
08/02 06:36:56 AM | ALPHA_WEIGHT_DECAY=0.001
08/02 06:36:56 AM | BATCH_SIZE=32
08/02 06:36:56 AM | DATA_PATH=/dataset/CIFAR10
08/02 06:36:56 AM | DATASET=CIFAR10
08/02 06:36:56 AM | EPOCHS=100
08/02 06:36:56 AM | FEATURE_EPOCH=0
08/02 06:36:56 AM | GPUS=[0, 1, 2, 3]
08/02 06:36:56 AM | INIT_CHANNELS=16
08/02 06:36:56 AM | LAYERS=8
08/02 06:36:56 AM | NAME=0802_darts_cluster
08/02 06:36:56 AM | PATH=searchs/0802_darts_cluster
08/02 06:36:56 AM | PLOT_PATH=searchs/0802_darts_cluster/plots
08/02 06:36:56 AM | PRINT_FREQ=50
08/02 06:36:56 AM | SAVE_DIR=/searchs/
08/02 06:36:56 AM | SEED=2
08/02 06:36:56 AM | W_GRAD_CLIP=3.0
08/02 06:36:56 AM | W_LR=0.0375
08/02 06:36:56 AM | W_LR_MIN=0.0015
08/02 06:36:56 AM | W_MOMENTUM=0.9
08/02 06:36:56 AM | W_WEIGHT_DECAY=0.0003
08/02 06:36:56 AM | WORKERS=4
08/02 06:36:56 AM | 
08/02 06:36:56 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.0908, 0.0910, 0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0911, 0.0908, 0.0908, 0.0909, 0.0910,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0910, 0.0908,
         0.0909, 0.0909],
        [0.0908, 0.0911, 0.0910, 0.0909, 0.0910, 0.0908, 0.0910, 0.0907, 0.0909,
         0.0909, 0.0909],
        [0.0910, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0910, 0.0911, 0.0908, 0.0909, 0.0908, 0.0909, 0.0909, 0.0910, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0909, 0.0908, 0.0909, 0.0911, 0.0910, 0.0909, 0.0911, 0.0909,
         0.0909, 0.0907],
        [0.0908, 0.0909, 0.0909, 0.0911, 0.0909, 0.0908, 0.0910, 0.0909, 0.0908,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0911, 0.0909, 0.0908, 0.0911, 0.0910, 0.0910, 0.0908, 0.0909, 0.0906,
         0.0909, 0.0910],
        [0.0909, 0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0910, 0.0908, 0.0909, 0.0910, 0.0911, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0910, 0.0910, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0910],
        [0.0909, 0.0908, 0.0908, 0.0910, 0.0908, 0.0910, 0.0909, 0.0909, 0.0911,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0909, 0.0908, 0.0908, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909, 0.0910,
         0.0911, 0.0910],
        [0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0910, 0.0908, 0.0911, 0.0908,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0910, 0.0909, 0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0910, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909,
         0.0910, 0.0909],
        [0.0909, 0.0909, 0.0907, 0.0911, 0.0909, 0.0909, 0.0910, 0.0910, 0.0909,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0910, 0.0908, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0908, 0.0910,
         0.0909, 0.0909],
        [0.0909, 0.0908, 0.0909, 0.0908, 0.0910, 0.0908, 0.0910, 0.0910, 0.0909,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0908, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0910, 0.0909, 0.0909, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910,
         0.0908, 0.0910],
        [0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0909, 0.0910, 0.0909, 0.0907, 0.0909, 0.0911, 0.0907, 0.0909, 0.0910,
         0.0909, 0.0909],
        [0.0908, 0.0908, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 06:38:15 AM | Train: [ 1/100] Step 000/781 Cluster Loss 2.346 Loss 4.655Prec@(1,5) (6.2%, 53.1%)
